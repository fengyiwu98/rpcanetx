<!--
  The index.html in github's repositories is different 
  The is difference is source's path
  The relative path in github should add "./"
-->
<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
 
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="RPCANet++">
  <!--
    //the keywords contains keywords relevant to your paper for search engine indexing
  -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--
    //the viewport ensures the page scales correctly on different devices
  -->

  <title>RPCANet++</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <!--
    used to identify image beside the browser title 
    标签页的名称
    标签页的图片
  -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/twentytwenty.css" type="text/css" media="screen" />

  <!-- For TwentyTwenty-->
  <script
  src="https://code.jquery.com/jquery-3.2.1.js"
  integrity="sha256-DZAnKJ/6XZ9si04Hgrsxu/8s717jcIzLy3oi35EouyE="
  crossorigin="anonymous"></script>
  <script src="static/js/jquery.event.move.js" type="text/javascript"></script>
  <script src="static/js/jquery.twentytwenty.js" type="text/javascript"></script>
  
  <!--The index.css have changed by my myself-->


  <script src="https://documentcloud.adobe.com/view-sdk/main.js">
    </script>
  <script defer src="static/js/fontawesome.all.min.js">
    </script>
  <script src="static/js/bulma-carousel.min.js">
    </script>
  <script src="static/js/bulma-slider.min.js">
    </script>
  <script src="static/js/index.js">
    </script>

  <!--MathJax-->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body data-new-gr-c-s-check-loaded="14.1104.0" data-gr-ext-installed>



<!-- The start of heading component-->
<section class="hero" style="margin: auto;">
    <div class="hero-body " style="margin: auto;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">RPCANet<sup>++</sup>: Deep Interpretable Robust PCA for Sparse Object Segmentation
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/fengyiwu98" target="_blank">Fengyi Wu</a><sup>1</sup>,</span>
              <span class="author-block">
                  <a href="https://github.com/YimianDai" target="_blank">Yimian Dai</a><sup>2</sup>,
                    </span>
              <span class="author-block">
                <a href="https://tianfang-zhang.github.io/" target="_blank">Tianfang Zhang</a><sup>3</sup>,
                </span>
              <span class="author-block">
                <a href="https://idiplab.uestc.cn/" target="_blank">Yixuan Ding</a><sup>1</sup>,
                  </span>
              <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=6CIDtZQAAAAJ&hl=zh-CN" target="_blank">Jian Yang</a><sup>2</sup>,
              </span> <span class="author-block">
                    <a href="https://mmcheng.net/cmm/comment-page-1/" target="_blank">Ming-Ming Cheng</a><sup>2</sup>,
                      </span>                  
              <span class="author-block">
                    <a href="https://idiplab.uestc.cn/" target="_blank">Zhenming Peng</a><sup>1</sup>
                      </span>
            </div>

            <div class="is-size-5 publication-authors">

              <span class="author-block"><sup>1</sup>
                UESTC, China<br>IDIP Lab, SICE</span>
              <span class="author-block"><sup>2</sup>
                Nankai University, China<br>VCIP Lab, CCS</span>
              <span class="author-block"><sup>3</sup>
                Tsinghua University, China<br>DA</span>

              <!--
                <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              -->
            </div>

             <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <!-- 
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Wait for Paper</span>
                    </a>
                  </span>
                  -->

                  <!-- Supplementary PDF link -->
                  <!-- 
                  <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Wait for Supplementary</span>
                    </a>
                  </span>
                  -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/fengyiwu98/RPCANet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>

                  <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="http://arxiv.org/abs/2508.04190" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>2508.04190</span>
                    </a>
                  </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- The end of heading component-->

<!-- The start of overview-->
<section class = "hero teaser">
  <div class = "container">
    <figure class = "imgae is-4by3" >
      <img src = "./static/images/hl1.png" alt = "Failure" style="border-radius: 15px; overflow: hidden;">
      <figcaption><b>Overview of the proposed RPCANet</a><sup>++</sup> architecture.</b> A. Model the given image within a relaxed RPCA scheme and transform it into an unconstrained optimization problem. B. Iteratively solves the model above with closed-form solutions; Consider two high-level issues with corresponding solutions. C. Unfold the solutions in a deep unfolding framework; typically, RPCANet</a><sup>++</sup> are assisted with memory-augmented modules and deep target priors. D. Visual and numerical model verifications via post-hoc techniques present overall interpretability.</figcaption>
    </figure>
  </div>
</section>
<!-- The end of overview-->

<!-- The start of test for gallery videos-->
<section class="hero is-light is-small" style="display: none;"> <!-- 流动视频播放这个板块我暂时把display关掉了-->
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <!-- For videos-->
        <div class="item">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/carousel1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/carousel2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/carousel3.mp4"
                    type="video/mp4">
          </video>
        </div>

        <!-- For images-->
        <div class="item">
          <img src="./static/images/carousel1.jpg" alt="描述图片内容" />
        </div>
        <div class="item">
          <img src="./static/images/carousel2.jpg" alt="另一张图片的描述" />
        </div>
        
      </div>
    </div>
  </div>
</section>
<!-- The end of test for gallery videos-->


<!-- The start of paper abstract -->
<section class="section hero is-light" style="background-color: rgb(237, 237, 237); margin-bottom: 50px; margin-top: 50px">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4by3">Abstract</h2>
        <div class = "section-divider"></div>
        <div class = "is-divider"></div>
        <div class="content has-text-justified" style="font-family: 'Times New Roman', Times, serif; font-size: larger;">
          <p>
            Robust principal component analysis (RPCA) decomposes an observation matrix into low-rank background and sparse object components. This capability has enabled its application in tasks ranging from image restoration to segmentation. However, traditional RPCA models suffer from computational burdens caused by matrix operations, reliance on finely tuned hyperparameters, and rigid priors that limit adaptability in dynamic scenarios. To solve these limitations, we propose RPCANet<sup>++</sup>, a sparse object segmentation framework that fuses the interpretability of RPCA with efficient deep architectures. Our approach unfolds a relaxed RPCA model into a structured network comprising a Background Approximation Module (BAM), an Object Extraction Module (OEM), and an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background feature preservation, while a Deep Contrast Prior Module (DCPM) leverages saliency cues to expedite object extraction. Extensive experiments on diverse datasets demonstrate that RPCANet<sup>++</sup> achieves state-of-the-art performance under various imaging scenarios. We further improve interpretability via visual and numerical low-rankness and sparsity measurements. By combining the theoretical strengths of RPCA with the efficiency of deep networks, our approach sets a new baseline for reliable and interpretable sparse object segmentation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- The end if paper abstract -->



<!-- The start of Pipeline-->
<section class="section pt-0">
<!-- The end of Pipeline-->
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3"> How it works </h2>
          <div class ="content has-text-justified">
            <h3 class="title has-text-centered"> Problem Modeling, Iterative Solving, and Unfolding to the Network </h3>
            <p>
              In the context of segmentation orientated RPCA tasks, our objective is to estimate low rank background \(\mathbf{B}\in \mathbb{R}^{m \times n}\) and extract the sparse object matrix \(\mathbf{O}\in \mathbb{R}^{m \times n}\). For an image \(\mathbf{D}\in \mathbb{R}^{m \times n}\), we transform the segmentation model into the following optimization framework:
              
              \begin{equation}
              \min \limits_{\mathbf{B},\mathbf{O}} rank(\mathbf{B}) + \lambda \left\| \mathbf{O} \right\|_0 \quad s.t.~\mathbf{D} = \mathbf{B} + \mathbf{O} \enspace,
              \label{eq_RPCA}
              \end{equation}
              
              where we signify \(\lambda\) as a trade-off coefficient, and the term \({\left\| \cdot \right\|_0}\) denotes the \(l_0\)-norm, which is defined as the count of non-zero elements within a matrix.
              However, when facing complex scenarios, the background can exhibit varying degrees of complexity, rendering a solitary nuclear norm or rank function insufficient for encapsulating the practical constraints. Similarly, the sparsity of object elements can vary, making the exclusive use of the \(l_0\) or \(l_1\)-norm potentially inadequate. Consequently, we propose a more generalized formulation of the problem. Here, we employ \(\mathcal{R}(\mathbf{B})\) and \(\mathcal{S}(\mathbf{O})\) as constraints that incorporate prior knowledge of the background and object images, individually:

              \begin{equation}
                  \min \limits_{\mathbf{B},\mathbf{O}} \mathcal{R}(\mathbf{B}) + \lambda \mathcal{S}(\mathbf{O}) \quad s.t.~\mathbf{D} = \mathbf{B} + \mathbf{O} \enspace.
              \label{eq_relaxPCP}
              \end{equation}  
            
            </p>
            <p>
          This motivate us to solve the above optimization problem in an iterative manner and unfolds the above optimization problem into a deep network as follows:
        </p>
            <img id="method_a2n" width="100%" src="./static/images/a2n.png" alt="Marigold training scheme">

            <br>
            <br>
            <br>
            <p>
              \(\textbf{RPCANet}^{++}\) framework unfolds iterative model-driven closed-form equations in deep network design and comprises corresponding \(K\) stages. Transmissive elements are presented in different colors: \(\mathbf{D}\) for the restoration image, \(\mathbf{B}\) for the low rank background, \(\mathbf{O}\) for the sparse object matrix, \(\rho\) for the learnable parameter, and \([\mathcal{B}_h,\mathcal{B}_c]\) for the latent background features.
              </p>
            <img id="method_overall" width="100%" src = "./static/images/overall.png" alt="Marigold training scheme">
            <br>

            <img id="method_detail" width="100%" src = "./static/images/detail.png" alt="Marigold training scheme">


<!-- The end of Pipeline-->
            <h3 class="title has-text-centered"> Model Verifications </h3>

            <p>
              In the context of deep unfolding, the network is architectured to iteratively yield guided results congruent with an algorithm’s unrolled stages. Demonstrating outcomes at each stage is vital for model validation. 
            </p>
            
            <img src = "./static/images/dataset_distri.png" alt = "Failure">

            <figcaption style="text-align: center">(a) Typcial sparse object segmentation tasks solved by RPCA methods with overall low-rank background. (b) Datasets utilized in this paper with objects' average area. </figcaption>
            <br>
            <br>
            <br>
            
            <img src = "./static/images/visualization.png" alt = "Failure">

            <figcaption>\(\textbf{Heatmaps}\) of different stages' \(\textbf{B}^{k}\) and \(\textbf{O}^{k}\) visualization results (\(K=6\)) of our RPCANet\(^{++}\) on various scenarios from six different datasets (\(\textbf{IRSTD}\), \(\textbf{VS}\), and \(\textbf{DD}\) tasks). We can observe its gradual shaping process via iterative unfolding.</figcaption>
            <br>
            <br>

            <img src = "./static/images/stages.png" alt = "Failure">

            <figcaption>The impact of different stage index \(K\) on detection efficacy</figcaption>
            <br>
            <br>



            <img src = "./static/images/inter_lowrank.png" alt = "Failure">

            <figcaption>
              \(\textbf{Low-rankness verification}\) of different stage features (1st to 6th) in \(\textbf{(a)}\) RPCANet\(^{++}\), compared to original images. As well as its variants \(\textbf{(b)}\) without MAM or \(\textbf{(c)}\) without DCPM, and the baseline \(\textbf{(d)}\) RPCANet. Verification is conducted on the IRSTD-1K test set. Our RPCANet\(^{++}\) progressively estimates background features satisfying low-rankness, step-by-step, without overestimation. [Zoom in for a better view]
            </figcaption>
            <br>
            <br>





            <img src="./static/images/inter_sparse_new.png" alt="Failure" style="width: 100%; height: auto;">
            <figcaption>\(\textbf{Sparsity verification}\) of different stages our RPCANet\(^{++}\) and its variants(without MAM or DCPM) vs RPCANet on IRSTD-1K. \(\textbf{Left:}\) numerical verification. \(\textbf{Right:}\) heatmaps among different stages.</figcaption>







            <h3 class="title has-text-centered"> Experimental Comparisons </h3>





<!-- The start of Quantitative Results-->


            <p>
              Performance metrics, including IoU (%), F1 (%), Pd (%), Fa (10<sup>-5</sup> ), and runtime are evaluated for various methods on datasets
              NUDT-SIRST, IRSTD-1K, SIRST, and SIRST-AUG. Parameter statistics for data-driven approach are encapsulated within the second column (Find more details in the main manuscript).
            </p>


            <img src = "./static/images/sota.png" alt = "Failure">

            <br>
            <br>

<!-- The end of Quantitative Results-->

<!-- The start of Quanlitative Results-->
              <h3 style="text-align: center; font-weight: bold; font-size: 1em;">Infrared Small Target Detection (IRSTD) task</h3>


              <img src = "./static/images/visual_result.png" alt = "Failure">

              <h3 style="text-align: center; font-weight: bold; font-size: 1em;">Vessel Segmentation (VS) task</h3>


              <img src = "./static/images/medical.png" alt = "Failure">



              <h3 style="text-align: center; font-weight: bold; font-size: 1em;">Defect Detection (DD) task</h3>
            
              <img src = "./static/images/defect.png" alt = "Failure">
              <p>
              Refer to the arxiv paper link above for more details on qualitative, quantitative, and ablation studies.
            </p>
  </div>
</div>
</div>
</div>
</section>
<!-- The end of Quantitative Results-->

  <!-- The start of gif gallery section-->
<section class="hero is-light is-small" style="background-color: rgb(255, 255, 255);"> <!-- classes from Bulma for a light background and smaller height -->
  <div class="hero-body">
    <div class="container" style="background-color: #ffffff;">
      <h2 class = "title is-4by3 has-text-centered" style="color: rgb(51, 50, 50);" >Gallery</h2>
      <div class = "section-divider"></div>
      <div class = "content my-content-front has-text-centered" s>
        <p>
          Heatmap results of different stages' \(\mathbf{O}^{k}\) of our RPCANet</a><sup>++</sup> on various scenarios from the IRSTD task.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel" style="background-color: #ffffff; justify-items: center;">
        <!-- First module -->
        <div class="carousel-item-8" style="position: relative; ">
          <div class="image-box" >
            <img src="./static/images/000516.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000536.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000595.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000597.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000654.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/001101.png" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/516.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/536.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/595.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/597.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/654.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/1101.gif" /> 
          </div>
        </div>
          <!-- Second module -->
        <div class="carousel-item-8" style="position: relative; ">
          <div class="image-box" >
            <img src="./static/images/000187.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000273.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000351.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000386.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000410.png" /> 
          </div>
          <div class="image-box" >
            <img src="./static/images/000492.png" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/187.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/273.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/351.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/386.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/410.gif" /> 
          </div>
          <div class="gif-box">
            <img src="./static/images/492.gif" /> 
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section pt-0 pb-2">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"> Related and Follow-up Works </h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <a href="https://github.com/fengyiwu98/RPCANet">RPCANet</a>, WACV 2024, an early version of our work on the IRSTD task.
            </li>
            <li>
              <a href="https://github.com/GrokCV/DRPCA-Net">DRPCA-Net</a>, TGRS 2025, follow up work on the IRSTD task.
            </li>
            </ul>
          </div>
        </div>
      </div>



  
      <section class="section">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title has-text-centered">Our Talks</h2>
                <div class="columns is-multiline">
                    <!-- First Video -->
                    <div class="column is-half">
                        <div class="box">
                            <figure class="image is-16by9">
                              <iframe class="has-ratio" width="640" height="360" src="https://player.bilibili.com/player.html?isOutside=true&aid=1155704349&bvid=BV1JZ421g7K5&cid=1574662214&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="true"></iframe> 
                            </figure>
                            <p class="mt-3">CHN Version</p>
                        </div>
                    </div>
                    <!-- Second Video -->
                    <div class="column is-half">
                        <div class="box">
                            <figure class="image is-16by9">
                                <iframe class="has-ratio" width="640" height="360" src="https://www.youtube.com/embed/pyYy7_7Fw3g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            </figure>
                            <p class="mt-3">ENG Version</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


<!-- The start of BibTex citation -->
  <section class="section" id="Citation" style=" margin-top: 10px">
    <div>
      <h2 class="title">Citation</h2>
      <pre><code>@misc{wu2025rpcanet_pp,
      title={RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation}, 
      author={Fengyi Wu and Yimian Dai and Tianfang Zhang and Yixuan Ding and Jian Yang and Ming-Ming Cheng and Zhenming Peng},
      year={2025},
      eprint={2508.04190},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.04190}, 
}</code></pre>
      <pre><code>@InProceedings{Wu_2024_WACV,
        author    = {Wu, Fengyi and Zhang, Tianfang and Li, Lei and Huang, Yian and Chen, Mingming and Peng, Zhenming},
        title     = {RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection},
        booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
        month     = {January},
        year      = {2024},
        pages     = {4809-4818}
    }</code></pre>

    </div>
</section>
<!-- The end of BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
